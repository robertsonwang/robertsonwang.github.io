{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Download the Python module](https://robertsonwang.github.io/yelp_ml.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import svm\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import math\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "#################################\n",
    "#Declare import word dictionaries\n",
    "#################################\n",
    "\n",
    "lh_neg = open('../input/negative-words.txt', 'r').read()\n",
    "lh_neg = lh_neg.split('\\n')\n",
    "lh_pos = open('../input/positive-words.txt', 'r').read()\n",
    "lh_pos = lh_pos.split('\\n')\n",
    "\n",
    "pos_vectorizer = CountVectorizer(vocabulary = lh_pos)\n",
    "neg_vectorizer = CountVectorizer(vocabulary = lh_neg)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#################################\n",
    "#Plotting functions\n",
    "#################################\n",
    "\n",
    "def label_point(x, y, ax):\n",
    "    a = pd.concat({'x': x, 'y': y}, axis=1)\n",
    "    a = a[a['y'] != max(a['y'])]\n",
    "    \n",
    "    for i, point in a.iterrows():\n",
    "        if (point['y'] > (a['y'].mean() + 1.5 * a['y'].std()) ):\n",
    "            ax.text(point['x'], point['y'], int(point['x']), \n",
    "                    verticalalignment='bottom', horizontalalignment='left')\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "def plot_coefficients(classifier, feature_names, top_features=20):\n",
    "    coef = classifier.coef_.ravel()\n",
    "    top_positive_coefficients = np.argsort(coef)[-top_features:]\n",
    "    top_negative_coefficients = np.argsort(coef)[:top_features]\n",
    "    top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])\n",
    "    # create plot\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    colors = ['red' if c < 0 else 'blue' for c in coef[top_coefficients]]\n",
    "    plt.bar(np.arange(2 * top_features), coef[top_coefficients], color=colors)\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.xticks(np.arange(1, 1 + 2 * top_features), feature_names[top_coefficients], \n",
    "               rotation=60, ha='right')\n",
    "    plt.show()\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words, top_topics):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        if topic_idx in top_topics:\n",
    "            print \"Topic %d:\" % (topic_idx)\n",
    "            print \" \".join([feature_names[i]\n",
    "                            for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "        else:\n",
    "            pass\n",
    "#################################\n",
    "#Feature objects and functions\n",
    "#################################\n",
    "\n",
    "def sent_percent(review):\n",
    "    regex_words = re.compile('[a-z]+')\n",
    "    words = [x.lower() for x in review.split(' ')]\n",
    "    words = [x for x in words if regex_words.match(x)]\n",
    "    pos_count, neg_count = 0, 0\n",
    "    for word in words:\n",
    "        if word in lh_pos:\n",
    "            pos_count += 1\n",
    "        elif word in lh_neg:\n",
    "            neg_count += 1\n",
    "    return [float(pos_count)/float(len(words)), float(neg_count)/float(len(words))]\n",
    "\n",
    "\n",
    "class SentimentPercentage(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, reviews):\n",
    "        ##Take in a list of textual reviews and return a list with two elements:\n",
    "        ##[Positive Percentage, Negative Percentage]\n",
    "        pos_vect = pos_vectorizer.transform(reviews)\n",
    "        neg_vect = neg_vectorizer.transform(reviews)\n",
    "        features = []\n",
    "        \n",
    "        for i in range(0, len(reviews)):\n",
    "            sent_percentage = []\n",
    "            sent_percentage.append(float(pos_vect[i].sum())/float(len(reviews[i])))\n",
    "            sent_percentage.append(float(neg_vect[i].sum())/float(len(reviews[i])))\n",
    "            features.append(sent_percentage)\n",
    "            \n",
    "        return np.array(features)\n",
    "\n",
    "    def fit(self, reviews, y=None, n_grams = None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "class TfIdfGramTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, reviews):\n",
    "        tf_vector = vectorizer.transform(reviews)\n",
    "        return tf_vector\n",
    "\n",
    "    def fit(self, reviews, y=None, n_grams = (2,2)):\n",
    "        vectorizer = TfidfVectorizer(ngram_range = n_grams, stop_words = 'english')\n",
    "        vectorizer.fit(reviews)\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return vectorizer\n",
    "\n",
    "def get_restaurant_reviews(ip, business_ids):\n",
    "    #Input: IP is the ip to the AWS instance that has MongoDB running\n",
    "    #business_ids is a list of unique businesses IDs for which the user has created reviews\n",
    "    #Output: A dictionary where the restaurant IDs are the keys and the entries to each key is the list of reviews for that business\n",
    "\n",
    "    conn = MongoClient(ip, 27017)\n",
    "    conn.database_names()\n",
    "    db = conn.get_database('cleaned_data')\n",
    "    reviews = db.get_collection('restaurant_reviews')\n",
    "\n",
    "    restreview = {}\n",
    "\n",
    "    for i in range(0, len(business_ids)):\n",
    "        rlist = []\n",
    "        for obj in reviews.find({'business_id':business_ids[i]}):\n",
    "            rlist.append(obj)\n",
    "        restreview[business_ids[i]] = rlist\n",
    "\n",
    "    return restreview\n",
    "\n",
    "def fit_lsi(train_reviews):\n",
    "    #Input: train_reviews is a list of reviews that will be used to train the LSI feature transformer\n",
    "    #Output: A trained LSI model and the transformed training reviews\n",
    "\n",
    "    texts = [[word for word in review.lower().split() if (word not in stop_words)]\n",
    "              for review in train_reviews]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    numpy_matrix = matutils.corpus2dense(corpus, num_terms=10000)\n",
    "    singular_values = np.linalg.svd(numpy_matrix, full_matrices=False, compute_uv=False)\n",
    "    mean_sv = sum(list(singular_values))/len(singular_values)\n",
    "    topics = int(mean_sv)\n",
    "\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=topics)\n",
    "\n",
    "    return lsi, topics, dictionary\n",
    "\n",
    "def make_featureunion(sent_percent=True, tf = True, lda = True):\n",
    "    #Input: sent_percent, tf, and lda are all boolean variables and indicate which features should be used in the ML algorithm\n",
    "    #sent_percent: The percentage of positive and negative words in a review, as defiend by H&L (2004), relative to the total words\n",
    "    #tf: tf-idf representation using a ngram range of (0,1)\n",
    "    #lda: LDA representation using an ngram range of (1,1)\n",
    "    #Output: A FeatureUnion object with the specified features horizontally stacked in a SciPy Sparse Matrix.\n",
    "\n",
    "    if sent_percent == False:\n",
    "        comb_features = FeatureUnion([('tf', TfIdfGramTransformer()),\n",
    "                              ('lda', Pipeline([('bow', TfidfVectorizer(stop_words='english', ngram_range = (1,1))), \n",
    "                                        ('lda_transform', LatentDirichletAllocation(n_topics=50))]))\n",
    "                             ])\n",
    "    elif tf == False:\n",
    "        comb_features = FeatureUnion([('sent_percent',SentimentPercentage()), \n",
    "                              ('lda', Pipeline([('bow', TfidfVectorizer(stop_words='english', ngram_range = (1,1))), \n",
    "                                        ('lda_transform', LatentDirichletAllocation(n_topics=50))]))\n",
    "                             ])\n",
    "    elif lda == False:\n",
    "        comb_features = FeatureUnion([('sent_percent',SentimentPercentage()),('tf', TfIdfGramTransformer())\n",
    "                             ])\n",
    "    else:\n",
    "        comb_features = FeatureUnion([('sent_percent',SentimentPercentage()),('tf', TfIdfGramTransformer()), \n",
    "                              ('lda', Pipeline([('bow', TfidfVectorizer(stop_words='english', ngram_range = (1,1))), \n",
    "                                        ('lda_transform', LatentDirichletAllocation(n_topics=50))])),\n",
    "                              ('tf_bow', TfidfVectorizer(stop_words='english'))\n",
    "                             ])\n",
    "\n",
    "    return comb_features\n",
    "\n",
    "\n",
    "def fit_model(train_features, train_labels, svm_clf = False, RandomForest = False, nb = False):\n",
    "    #Input: SVM, RandomForest, and NB are all boolean variables and indicate which model should be fitted\n",
    "    #SVM: Linear Support Vector Machine\n",
    "    #RandomForest: Random Forest, we set the max_depth equal to 50 because of prior tests\n",
    "    #NB: LDA representation using an ngram range of (1,1)\n",
    "    #train_features: Train reviews that have been transformed into the relevant features\n",
    "    #train_labels: Labels for the training reviews, transformed into a binary variable\n",
    "    #Output: A fitted model object\n",
    "\n",
    "    if svm_clf == True:\n",
    "        clf = svm.LinearSVC()\n",
    "        clf.fit(train_features, train_labels)\n",
    "        return clf\n",
    "    elif RandomForest == True:\n",
    "        clf = RandomForestClassifier(max_depth = 100, max_leaf_nodes=50, criterion='entropy')\n",
    "        clf.fit(train_features, train_labels)\n",
    "        return clf\n",
    "    elif nb == True:\n",
    "        clf = GaussianNB()\n",
    "        clf.fit(train_features, train_labels)\n",
    "        return clf\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def fit_features(train_reviews, comb_features):\n",
    "    #Input:\n",
    "    #train_reviews: A list of reviews to be fitted upon \n",
    "    #comb_features: A FeatureUnion object with the relevant features specified\n",
    "    #Output: A fitted FeatureUnion object\n",
    "\n",
    "    if not train_reviews:\n",
    "        return None\n",
    "    else:\n",
    "        comb_features = comb_features.fit(train_reviews)\n",
    "\n",
    "    return comb_features\n",
    "\n",
    "def get_lsi_features(reviews, lsi, topics, dictionary):\n",
    "    #Input:\n",
    "    #reviews: A list of reviews to be transformed\n",
    "    #lsi: A fitted LSI model \n",
    "    #topics: An integer, number of topics that the LSI model was fitted upon\n",
    "    #Output: A matrix of features that have been transformed according to the LSI model\n",
    "\n",
    "    if not reviews:\n",
    "        return None\n",
    "    else:\n",
    "        texts = [[word for word in review.lower().split() if (word not in stop_words)]\n",
    "              for review in reviews]\n",
    "        corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "        tfidf = models.TfidfModel(corpus)\n",
    "        reviews_tfidf = tfidf[corpus]\n",
    "        reviews_lsi = lsi[reviews_tfidf]\n",
    "        reviews_lsi = [[text[1] for text in review] for review in reviews_lsi]\n",
    "        reviews_lsi = [[0.000000000001] * topics if len(x) != topics else x for x in reviews_lsi]\n",
    "        reviews_lsi = sparse.coo_matrix(reviews_lsi)\n",
    "\n",
    "    return reviews_lsi\n",
    "\n",
    "def get_transformed_features(comb_features, reviews):\n",
    "    #Input: \n",
    "    #comb_features: A FeatureUnion object which will be used to create a stacked feature matrix\n",
    "    #reviews: A list of reviews to be transformed\n",
    "    #Output: A feature matrix of the reviews, transformed according to the features in the FeatureUnion object\n",
    "\n",
    "    if not reviews:\n",
    "        return None\n",
    "    else:\n",
    "        transformed_reviews = comb_features.transform(reviews)\n",
    "\n",
    "    return transformed_reviews\n",
    "\n",
    "def make_biz_df(user_id, restreview):\n",
    "    #Input: \n",
    "    #user_id: A specific customer ID\n",
    "    #restreview: A dictionary where the keys are the restaurant IDs and entries \n",
    "    #are a list of the reviews for that restaurant\n",
    "    #Output: A dataframe with the columns (review_text, rest_ratings, biz_ids)\n",
    "    rest_reviews = []\n",
    "    rest_ratings = []\n",
    "    biz_ids = []\n",
    "    for i in range(0, len(restreview.keys())):\n",
    "        for restaurant in restreview[restreview.keys()[i]]:\n",
    "            if restaurant['user_id'] != user_id:\n",
    "                rest_reviews.append(restaurant['text'])\n",
    "                rest_ratings.append(restaurant['stars'])\n",
    "                biz_ids.append(restreview.keys()[i])\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    # numbers = re.compile(\"\\d+\")\n",
    "    # rest_reviews = [' '.join([word for word in review.lower().split() if not numbers.match(word)])\n",
    "    #           for review in rest_reviews]\n",
    "\n",
    "    rest_reviews = [review.replace(\".\", \" \") for review in rest_reviews]\n",
    "    rest_reviews = [review.replace(\"\\n\", \"\") for review in rest_reviews]\n",
    "    rest_reviews = [review.encode('utf-8').translate(None, string.punctuation) for review in rest_reviews]\n",
    "\n",
    "    biz_df = pd.DataFrame({'review_text': rest_reviews, 'rating': rest_ratings, 'biz_id': biz_ids})\n",
    "\n",
    "    return biz_df\n",
    "\n",
    "\n",
    "def make_user_df(user_specific_reviews):\n",
    "    #Input: \n",
    "    #user_specific_reviews: A list of reviews for a specific user\n",
    "    #Output: A dataframe with the columns (user_reviews, user_ratings, biz_ids)\n",
    "    user_reviews = []\n",
    "    user_ratings = []\n",
    "    business_ids = []\n",
    "\n",
    "    for review in user_specific_reviews:\n",
    "        user_reviews.append(review['text'])\n",
    "        user_ratings.append(review['stars'])\n",
    "        business_ids.append(review['business_id'])\n",
    "\n",
    "    #Make numbers regex rule\n",
    "    # numbers = re.compile(\"\\d+\")\n",
    "    # user_reviews = [' '.join([word for word in review.lower().split() if not numbers.match(word)])\n",
    "    #           for review in user_reviews]\n",
    "\n",
    "    user_reviews = [review.replace(\".\", \" \") for review in user_reviews]\n",
    "    user_reviews = [review.replace(\"\\n\", \" \") for review in user_reviews]\n",
    "    user_reviews = [review.encode('utf-8').translate(None, string.punctuation) for review in user_reviews]\n",
    "        \n",
    "    user_df = pd.DataFrame({'review_text': user_reviews, 'rating': user_ratings, 'biz_id': business_ids})\n",
    "    return user_df\n",
    "\n",
    "def test_user_set(test_set, clf, restaurant_df, users_df, comb_features, threshold, lsi = None, topics = None, dictionary = None, delta_tfidf = None):\n",
    "    #Input: \n",
    "    #test_set: The set of restaurant IDs, split from the users total set, on which we will test our classifier\n",
    "    #clf: Classifier trained on the fully stacked features \n",
    "    #restaurant_df: A dataframe of the restaurants in the test_set and the reviews associated with each restaurant\n",
    "    #users_df: A dataframe of the users reviews with the tuple (user, restaurant id, restaurant rating, review)\n",
    "    #comb_features: A FeatureUnion object that has been trained on the user's other reviews\n",
    "    #threshold: A float value \n",
    "    #Note: lsi and topics should be GLOBAL variables after running fit_lsi\n",
    "    #Output: A list of errors on predicting whether or not the user likes the restaurant in the test set\n",
    "    comb_error = []\n",
    "    for i in range(0,len(test_set)):\n",
    "        predicted_rating = 0\n",
    "        #Get reviews for that restaurant\n",
    "        test_reviews =[]\n",
    "        \n",
    "        test_reviews.extend(list(restaurant_df[restaurant_df['biz_id'] == test_set[i]]['review_text']))\n",
    "        #Transform features\n",
    "        test_features = comb_features.transform(test_reviews)\n",
    "        #LSI Features\n",
    "        \n",
    "\n",
    "        #Stack the features\n",
    "        if lsi == None or topics == None or dictionary == None:\n",
    "            stacked_test_features = test_features.todense()\n",
    "        elif delta_tfidf != None:\n",
    "            test_lsi = get_lsi_features(test_reviews, lsi, topics, dictionary)\n",
    "            test_delta_tfidf = delta_tfidf.transform(test_reviews)\n",
    "            stacked_test_features = sparse.hstack((test_features, test_lsi, test_delta_tfidf))\n",
    "            stacked_test_features =  stacked_test_features.todense()\n",
    "        else:\n",
    "            test_lsi = get_lsi_features(test_reviews, lsi, topics, dictionary)\n",
    "            stacked_test_features = sparse.hstack((test_features, test_lsi))\n",
    "            stacked_test_features =  stacked_test_features.todense()\n",
    "\n",
    "        #Get ML prediction\n",
    "        test_prediction = clf.predict(stacked_test_features)\n",
    "\n",
    "        if test_prediction.mean() >= threshold:\n",
    "            predicted_rating = 1\n",
    "\n",
    "        actual_rating = list(users_df[users_df['biz_id'] == test_set[i]]['rating'])[0]\n",
    "\n",
    "        if actual_rating >= 4:\n",
    "            actual_rating = 1\n",
    "        else:\n",
    "            actual_rating = 0\n",
    "\n",
    "        comb_error.append((test_prediction, predicted_rating, actual_rating))\n",
    "    return comb_error\n",
    "\n",
    "def make_rec(restaurants, clf, threshold, comb_features, lsi = None, topics = None, dictionary = None):\n",
    "    #Input: \n",
    "    #restaurants: The set of restaurant IDs, split from the users total set, on which we will test our classifier\n",
    "    #clf: Classifier trained on the fully stacked features \n",
    "    #restaurant_df: A dataframe of the restaurants in the test_set and the reviews associated with each restaurant\n",
    "    #users_df: A dataframe of the users reviews with the tuple (user, restaurant id, restaurant rating, review)\n",
    "    #comb_features: A FeatureUnion object that has been trained on the user's other reviews\n",
    "    #threshold: A float value \n",
    "    #Note: lsi and topics should be GLOBAL variables after running fit_lsi\n",
    "    #Output: A list of errors on predicting whether or not the user likes the restaurant in the test set\n",
    "    test_results = []\n",
    "    for i in range(0,len(restaurants.keys())):\n",
    "        predicted_rating = 0\n",
    "        #Get reviews for that restaurant\n",
    "        test_reviews = []\n",
    "        \n",
    "        for review in restaurants[restaurants.keys()[i]]['review']:\n",
    "            test_reviews.append(review['description'])\n",
    "        if len(test_reviews) >= 20:\n",
    "             #Transform features\n",
    "            test_features = comb_features.transform(test_reviews)\n",
    "\n",
    "            #Stack the features\n",
    "            if lsi == None or topics == None or dictionary == None:\n",
    "                stacked_test_features = test_features.todense()\n",
    "            else:\n",
    "                test_lsi = get_lsi_features(test_reviews, lsi, topics, dictionary)\n",
    "                stacked_test_features = sparse.hstack((test_features, test_lsi))\n",
    "                stacked_test_features = stacked_test_features.todense()\n",
    "\n",
    "            #Get ML prediction\n",
    "            test_prediction = clf.predict(stacked_test_features)\n",
    "\n",
    "            if test_prediction.mean() >= threshold:\n",
    "                predicted_rating = 1\n",
    "\n",
    "            test_results.append((test_reviews, restaurants.keys()[i], \n",
    "                test_prediction.mean(), predicted_rating))\n",
    "        else:\n",
    "            continue\n",
    "    return test_results\n",
    "\n",
    "def get_top_ten_recs(test_predictions):\n",
    "    #Input: \n",
    "    #test_predictions: A list of tuples in the form (Confidence, Classification Prediction) for the user's set of test restaurants\n",
    "    #Output: A list of the top 10 restaurants that the classification algorithm is most confident in\n",
    "    if test_predictions:\n",
    "        confidence_tuple = [(float(sum(list(x[0])))/float(len(x[0])),x[1]) for x in test_predictions]\n",
    "        confidence_tuple.sort()\n",
    "        top_ten = confidence_tuple[-10:]\n",
    "        return top_ten\n",
    "    else:\n",
    "        print \"Empty List Passed\"\n",
    "        return None\n",
    "\n",
    "def get_log_loss(test_predictions):\n",
    "    #Input: \n",
    "    #test_predictions: A list of tuples in the form (Confidence, Classification Prediction) for the user's set of test restaurants\n",
    "    #Output: The log loss score associated with each classifier\n",
    "    if test_predictions:\n",
    "        test_predictions = [(np.append(x[0],0.0000000000001),x[1], x[2]) if x[0].mean() == 0.0 else x for x in test_predictions]\n",
    "        test_predictions = [(np.append(x[0],0.0000000000001),x[1], x[2]) if x[0].mean() == 1 else x for x in test_predictions]\n",
    "        raw_log = [x[1] * math.log(x[0].mean()) + (1-x[2]) * math.log(1-x[0].mean()) for x in test_predictions]\n",
    "        log_loss = float(sum(raw_log))/float(len(raw_log))\n",
    "        return log_loss\n",
    "    else:\n",
    "        print \"Empty List Passed\"\n",
    "        return None\n",
    "\n",
    "def get_precision_score(test_predictions):\n",
    "    #Input: \n",
    "    #test_predictions: A list of tuples in the form (Confidence, Classification Prediction) for the user's set of test restaurants\n",
    "    #Output: The accuracy score associated with each classifier\n",
    "    if test_predictions:\n",
    "        true_positive = [x for x in test_predictions if (x[1] == x[2]) & (x[1] == 1)]\n",
    "        false_positive = [x for x in test_predictions if (x[1] != x[2]) & (x[1] == 1)]\n",
    "        try:\n",
    "            precision_score = float(len(true_positive))/float(len(true_positive) + len(false_positive))\n",
    "        except:\n",
    "            precision_score = 0.0\n",
    "        return precision_score\n",
    "    else:\n",
    "        print \"Empty List Passed\"\n",
    "        return None\n",
    "\n",
    "def get_accuracy_score(test_predictions):\n",
    "    #Input: \n",
    "    #test_predictions: A list of tuples in the form (Confidence, Classification Prediction) for the user's set of test restaurants\n",
    "    #Output: The accuracy score associated with each classifier\n",
    "    if test_predictions:\n",
    "        test_errors = [abs(x[1]-x[2]) for x in test_predictions]\n",
    "        accuracy_score = float(sum(test_errors))/float(len(test_errors))\n",
    "        return accuracy_score\n",
    "    else:\n",
    "        print \"Empty List Passed\"\n",
    "        return None\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
