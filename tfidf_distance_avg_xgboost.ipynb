{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost using TF-IDF and Word Distance Features\n",
    "\n",
    "Note, this resulted in a log-loss score of 0.39 on the test data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Load Required Python Libraries\n",
    "##########################################\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pylev import levenshtein\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "import chardet\n",
    "import itertools\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from gensim.models import word2vec, KeyedVectors\n",
    "from scipy.stats import kurtosis\n",
    "from sklearn.externals import joblib\n",
    "import xgboost as xgb\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Loads in Quora Dataset\n",
    "##########################################\n",
    "#Training Dataset\n",
    "data = pd.read_csv('train.csv')\n",
    "data['question1'] = data['question1'].astype(str)\n",
    "data['question2'] = data['question2'].astype(str)\n",
    "y = data['is_duplicate']\n",
    "df_train = data\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word_lengths function returns the number of common words between two questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_lengths(q1,q2):\n",
    "    data = pd.concat([q1, q2], axis=1)\n",
    "\n",
    "    #Length of Question\n",
    "    data['len_q1'] = data.question1.apply(lambda x: len(str(x)))\n",
    "    data['len_q2'] = data.question2.apply(lambda x: len(str(x)))\n",
    "\n",
    "    #Feature: Difference in length between the Questions\n",
    "    data['len_diff'] = data.len_q1 - data.len_q2\n",
    "    \n",
    "    #Word count of Question\n",
    "    data['len_word_q1'] = data.question1.apply(lambda x: len(str(x).split()))\n",
    "    data['len_word_q2'] = data.question2.apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    #Feature: Difference in length between the words\n",
    "    data['word_diff'] = data.len_word_q1 - data.len_word_q2\n",
    "    \n",
    "    #Feature: Common words between the Questions\n",
    "    data['len_common_words'] = data.apply(lambda x: len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
    "    final_word_features = [data['len_diff'], data['word_diff'], data['len_common_words']]\n",
    "    final_word_features = np.column_stack((np.array(data['len_diff']),\n",
    "                                          np.array(data['word_diff']),\n",
    "                                          np.array(data['len_common_words']),\n",
    "                                          ))\n",
    "    return final_word_features\n",
    "\n",
    "def shared_words(q1,q2):\n",
    "    question1_words = []\n",
    "    question2_words = []\n",
    "\n",
    "    for word in set(str(q1).lower().split()):\n",
    "        if word not in stop_words:\n",
    "            question1_words.append(word)\n",
    "\n",
    "    for word in set(str(q2).lower().split()):\n",
    "        if word not in stop_words:\n",
    "            question2_words.append(word)\n",
    "\n",
    "    #Question contains only stop words (or is an empty string)\n",
    "    if len(question1_words) == 0 or len(question2_words) == 0:\n",
    "        return 0\n",
    "\n",
    "    question1_shared_words = [w for w in question1_words if w in question2_words]\n",
    "    question2_shared_words = [w for w in question2_words if w in question1_words]\n",
    "\n",
    "    avg_words_shared = (len(question1_shared_words) + len(question2_shared_words))/(len(question1_words) + len(question2_words))\n",
    "    return avg_words_shared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These objects will be used to create each of the separate features we'll be feeding into our machine learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LevDistanceTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        \n",
    "        lev_distance_strings = [[a,b] \n",
    "        for a,b in zip(q1_list, q2_list)]\n",
    "        \n",
    "        lev_dist_array = np.array([\n",
    "    (float(levenshtein(pair[0], pair[1]))/\n",
    "    (float(sum([x.count('') for x in pair[0]])) + \n",
    "    float(sum([x.count('') for x in pair[1]])))) \n",
    "    for pair in lev_distance_strings \n",
    "        ])\n",
    "        \n",
    "        return lev_dist_array.reshape(len(lev_dist_array),1)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "class TfIdfDiffTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self, total_words):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        total_questions = q1_list + q2_list\n",
    "        total_questions = [x for x in total_questions if type(x) != float]\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(stop_words = 'english', vocabulary = total_words)\n",
    "        vectorizer.fit(total_questions)\n",
    "        tf_diff = vectorizer.transform(q1_list) - vectorizer.transform(q2_list)\n",
    "        return tf_diff\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "class CosineDistTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        total_questions = q1_list + q2_list\n",
    "        total_questions = [x for x in total_questions if type(x) != float]\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "        vectorizer.fit(total_questions)\n",
    "        \n",
    "        q1_tf = vectorizer.transform(q1_list) \n",
    "        q2_tf = vectorizer.transform(q2_list)\n",
    "        cos_sim = []\n",
    "        for i in range(0,len(q1_list)):\n",
    "            cos_sim.append(cosine_similarity(q1_tf[i], q2_tf[i])[0][0])\n",
    "            \n",
    "        return np.array(cos_sim).reshape(len(cos_sim),1)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "    \n",
    "class AverageSharedWords(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        avg_words = [shared_words(q1,q2) for q1, q2 in zip(q1_list, q2_list)]\n",
    "\n",
    "        return np.array(avg_words).reshape(len(avg_words),1)\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self\n",
    "\n",
    "class WordLengths(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in two lists of strings, extracts the lev distance between each string, returns list\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def transform(self, question_list):\n",
    "        q1_list = question_list[0]\n",
    "        q2_list = question_list[1]\n",
    "        word_len = word_lengths(q1_list, q2_list)\n",
    "        return word_len\n",
    "\n",
    "    def fit(self, question_list, y=None):\n",
    "        \"\"\"Returns `self` unless something different happens in train and test\"\"\"\n",
    "        return self  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a aggregated feature transformer using FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Combining all the features using FeatureUnion\n",
    "##########################################\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english')\n",
    "vectorizer.fit(df_train['question1'][0:5000] + df_train['question2'][0:5000])\n",
    "#vectorizer.fit(df_train['question1'] + df_train['question2'])\n",
    "total_words = list(set(vectorizer.get_feature_names()))\n",
    "\n",
    "comb_features = FeatureUnion([('tf', TfIdfDiffTransformer(total_words)), \n",
    "                              ('cos_diff',CosineDistTransformer()), \n",
    "                              ('lev', LevDistanceTransformer()),\n",
    "                              ('AvgWords', AverageSharedWords()),\n",
    "                              ('WordLengths', WordLengths())\n",
    "                             ])\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into testing and training using train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Split the dataset into training and testing datasets\n",
    "# ##########################################\n",
    "y = df_train.ix[:,'is_duplicate']\n",
    "all_features = comb_features.transform([df_train['question1'], df_train['question2']])\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, y, test_size=0.2, random_state=1317)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.68622\ttest-logloss:0.686336\n",
      "Multiple eval metrics have been passed: 'test-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-logloss hasn't improved in 100 rounds.\n",
      "[100]\ttrain-logloss:0.467485\ttest-logloss:0.477014\n",
      "[200]\ttrain-logloss:0.437951\ttest-logloss:0.451076\n",
      "[300]\ttrain-logloss:0.426012\ttest-logloss:0.441089\n",
      "[399]\ttrain-logloss:0.418002\ttest-logloss:0.434636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgboost_model_400iterations_8depth.pkl']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################\n",
    "# Running XGBoost\n",
    "##########################################\n",
    "# Set parameters for XGBoost\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 10\n",
    "\n",
    "d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "d_test = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_test, 'test')]\n",
    "\n",
    "bst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=100, verbose_eval=100)\n",
    "joblib.dump(bst, 'xgboost_model_400iterations_8depth.pkl')\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Loads in Quora Test Dataset\n",
    "##########################################\n",
    "#Test Dataset\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "#Replaces np.nan with ''\n",
    "df_test = df_test.replace(np.nan, '', regex=True)\n",
    "\n",
    "#Saves the cleaned test.csv\n",
    "df_test.to_csv('cleaned_test.csv')\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test_features.pkl']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################\n",
    "# Create the test features using FeatureUnion\n",
    "##########################################\n",
    "# test_features = comb_features.transform([df_test['question1'][0:5000], df_test['question2'][0:5000]])\n",
    "test_features = comb_features.transform([df_test['question1'], df_test['question2']])\n",
    "joblib.dump(test_features, 'test_features.pkl')\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Predicting using XGBoost\n",
    "##########################################\n",
    "test = xgb.DMatrix(test_features)\n",
    "test_prediction = bst.predict(test)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Creating Submission File\n",
    "##########################################\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = df_test['test_id']\n",
    "sub['is_duplicate'] = test_prediction\n",
    "\n",
    "##########################################\n",
    "# Set probability to 0 for all test questions \n",
    "# that we know are not duplicates\n",
    "##########################################\n",
    "empty_questions = list(df_test[df_test['question1'] == '']['test_id']) + list(df_test[df_test['question2'] == '']['test_id'])\n",
    "for question in empty_questions:\n",
    "    sub.loc[question, 'is_duplicate'] = 0\n",
    "\n",
    "sub.to_csv('simple_xgb.csv', index=False)\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the submission question length is the same as the test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Submission File Length\n",
    "len(sub) == len(df_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
