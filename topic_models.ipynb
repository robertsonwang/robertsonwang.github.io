{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "LDA models each document as a mixture over topics. Each topic is a distribution over a selection of words and each word within the corpus of docuemnts is assumed to be from a particular distribution of words. The algorithm takes the number of topics,  kk , as a parameter. Each  kk  is calle a latent topic. LDA produces the support for each topic distribution via Bayesian updating. In general LDA:\n",
    "Starts with each document being represented as a bag of words.\n",
    "Produces the word distribution for each topic using approximating methods\n",
    "Returns a vector of length  kk  for each document, where each entry is the weight per distribution for each document\n",
    "On a high level, we are interested in solving the following conditional probability:\n",
    "p(θ,z|w,α,β)=p(θ,z,w|α,β)p(w|α,β)p(θ,z|w,α,β)=p(θ,z,w|α,β)p(w|α,β) \n",
    "Where:\n",
    "θ is a parameter drawn from a Dirichlet distrubtion characterized by αz is a vector of topics, where each z is drawn from a multinomial distrubtion characterized by θw is a vector of words such that each w has multinomial probability p(w|z,β)θ is a parameter drawn from a Dirichlet distrubtion characterized by αz is a vector of topics, where each z is drawn from a multinomial distrubtion characterized by θw is a vector of words such that each w has multinomial probability p(w|z,β) \n",
    "Note that  αα  and  ββ  are user-specified parameters. Unfortunately, the above conditional probabilty does not have a closed form solution and we must use approximating methods to find a solution. Below is an outline for a popular method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collapsed Gibbs Sampling\n",
    "\n",
    "Go through each document D, and randomly assign each word in D to one of the  kk  topics.\n",
    "For each D, go through each word W in D, and for each topic  kk :\n",
    "Compute P( kk |D) = The probability of a topic given a document. This is the proportion of words in document D that are currently assigned to topic T\n",
    "Compute P(W| kk ) = The probability of a word given a topic. This is the proportion of document assignments to topic T over all documents that result from the word W.\n",
    "Assign W to a new topic. We choose the new topic by computing the probability P( kk |D) x P(W| kk ) for each topic and then pick the topic with the highest probability. This is the probability that topic T generated word W, holding everything else constant. In this step, we're assuming that all topic assignments except for the current word are correct, and then updating the assignment of the current word using our model of how documents are generated.\n",
    "Repeat steps 2 and 3 until we reach a rough steady state. That is, the support of the distribution of topic T (the underlying words in the distribution) are not changing more than some minimal threshold.\n",
    "After generating the topics, we can represent each document as a weight from each topic. Recall that we already represent the document as a bag of words, so we essentially combine the word counts for words in the same latent topic when we compute topic weights. That is, we compute the topic mixtures of each document by counting the proportion of words assigned to each topic within that document.\n",
    "See here for an application of this methodology in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
