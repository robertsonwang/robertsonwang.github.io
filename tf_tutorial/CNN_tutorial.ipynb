{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Download the Jupyter Notebook](https://robertsonwang.github.io/tf_tutorial/CNN_tutorial.ipynb)\n",
    "\n",
    "To install tensorflow, make sure you have the latest version of pip installed. Type pip install tensorflow at the command line.\n",
    "\n",
    "The below is a tutorial using code sourced from the following links:\n",
    "\n",
    "https://www.tensorflow.org/get_started/mnist/beginners\n",
    "https://www.tensorflow.org/get_started/mnist/pros\n",
    "\n",
    "And information sourced from the Ujjwal Karn's blog:\n",
    "\n",
    "https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/\n",
    "\n",
    "All MNIST data can be found at Yann Lecun's website:\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import gzip\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define file extraction function\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 10\n",
    "def extract_data(filename, num_images):\n",
    "#Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "#Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, 1)\n",
    "    return data\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "#Extract the labels into a vector of int64 label IDs.\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * num_images)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    return labels\n",
    "\n",
    "def make_one_hot(labels):\n",
    "    one_hot_labels = []\n",
    "    for array in labels:\n",
    "        hot_vect = [0] * 10\n",
    "        hot_vect[array] = 1\n",
    "        one_hot_labels.append(hot_vect)\n",
    "    one_hot_labels = np.array(one_hot_labels)\n",
    "    return one_hot_labels\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../tf_tutorial/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../tf_tutorial/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../tf_tutorial/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../tf_tutorial/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Load in MNIST training data\n",
    "train_data_filename = \"../tf_tutorial/MNIST_data/train-images-idx3-ubyte.gz\"\n",
    "train_labels_filename = \"../tf_tutorial/MNIST_data/train-labels-idx1-ubyte.gz\"\n",
    "test_data_filename = \"../tf_tutorial/MNIST_data/t10k-images-idx3-ubyte.gz\"\n",
    "test_labels_filename = \"../tf_tutorial/MNIST_data/t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "train_data = extract_data(train_data_filename, 55000)\n",
    "train_labels = extract_labels(train_labels_filename, 55000)\n",
    "test_data = extract_data(test_data_filename, 10000)\n",
    "test_labels = extract_labels(test_labels_filename, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build out a simple Softmax Neural Network (Logistic Regression)\n",
    "\n",
    "<img src=\"soft_max_reg.JPG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create an interactive tensorflow session\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#Set placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "#Declare variables\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "#Run the interactive session\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#Create prediction\n",
    "y = tf.matmul(x,W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set the cost function that the NN minimizes\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "#Train the model\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "hot_train_labels = make_one_hot(train_labels)\n",
    "for _ in range(1000):\n",
    "    batch = random.sample(zip(train_data,hot_train_labels) , 100)\n",
    "    image_batch = [a for (a,b) in batch]\n",
    "    image_batch = np.reshape(image_batch, (-1, 784))\n",
    "    label_batch = [b for (a,b) in batch]\n",
    "    sess.run(train_step, feed_dict={x: image_batch, y_: label_batch})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8383\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the simple softmax model\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "hot_test_labels = make_one_hot(test_labels)\n",
    "#Run the empty models above with our train and test data\n",
    "reshaped_test = np.reshape(test_data, (-1,784))\n",
    "print(sess.run(accuracy, feed_dict={x: reshaped_test, y_: hot_test_labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The image needs to be a 4d tensor\n",
    "x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "\n",
    "#First Convolutional Layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "#x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "#Second Convolutional Layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "#Densely Connected Layer\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "#Dropout - note that dropout is a technique to deal with overfitting. \n",
    "#The general idea is too randomly drop units (along with their connections) from the NN during training.\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "#Readout layer, this is simply the last layer in the network and combines the results from the second layer\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.08\n",
      "step 1000, training accuracy 0.96\n",
      "step 2000, training accuracy 0.96\n",
      "step 3000, training accuracy 0.9\n",
      "step 4000, training accuracy 1\n",
      "step 5000, training accuracy 0.98\n",
      "step 6000, training accuracy 0.96\n",
      "step 7000, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "test accuracy 0.9801\n"
     ]
    }
   ],
   "source": [
    "#Train and evaluate the model\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "hot_train_labels = make_one_hot(train_labels)\n",
    "hot_test_labels = make_one_hot(test_labels)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(10000):\n",
    "        batch = random.sample(zip(train_data,hot_train_labels), 50)\n",
    "        image_batch = [a for (a,b) in batch]\n",
    "        label_batch = [b for (a,b) in batch]\n",
    "        if i % 1000 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x: image_batch, y_: label_batch, keep_prob: 1.0})\n",
    "            print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "        train_step.run(feed_dict={x: image_batch, y_: label_batch, keep_prob: 0.5})\n",
    "    \n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={x: test_data, y_: hot_test_labels, keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So what's going on exactly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, convince yourself that every image can be represented as a matrix of numeric values.\n",
    "\n",
    "<img src=\"pixel_rep.gif\">\n",
    "\n",
    "We use the term channel to refer to the different dimensions that an image can take. For example, consider a color photograph. If we are only considering the colors, then each color takes on a value for each of three channels: red, green, and blue. This is represented by three stacked 2-dimensional matrices, each having numeric values in the range 0 to 255.\n",
    "\n",
    "Let us define a **convolution operator**:\n",
    "\n",
    "$(f*g)(t) = \\int_{0}^{t} f(\\tau) g(t-\\tau)d \\tau \\text{ for } f,g: [0, \\infty]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of a CNN, our convolutions are dot products across different subsection of the input matrix and a **filter** matrix. The different subsections of the input matrix are determined by **stride** size. Let's do an example below.\n",
    "\n",
    "Consider the following input matrix:\n",
    "\n",
    "<img src=\"input_mat.png\">\n",
    "\n",
    "Let the filter matrix be:\n",
    "\n",
    "<img src=\"filt_mat.png\">\n",
    "\n",
    "Set the stride size to be one so that we slide over the input matrix by 1 pixel for each convolution. We generate a 3x3 matrix from the input matrix starting at element [1,1], then we create another 3x3 matrix starting at the point [1,2]. We keep iterating until we can no-longer create clean 3x3 matrix subsets of the input matrix. This occurs at element [3,3]. Visually, our convolution looks like:\n",
    "\n",
    "<img src=\"convolution.gif\">\n",
    "\n",
    "We're left with a 3x3 matrix that we call the Convolved Feature/Activation Map/Feature Map. \n",
    "\n",
    "Different filters will result in different feature maps. See below for a quick example using a picture of a squirrel:\n",
    "\n",
    "<img src=\"convolved_squirrel.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, a CNN learns the values of these filters during the training process. The hyper parameters that we specify in learning include the number of filters, filter size, and architecture of the network (number of layers).\n",
    "\n",
    "Filter size is controlled by three hyper-parameters:\n",
    "1. Depth: The _number_ of filters we use, this determines the number of feature maps. \n",
    "2. Stride: The number of pixels by which we slide the filter matrix over the input matrix.\n",
    "3. Zero-padding: Whether or not to pad the outside edges of the input matrix with zeros. We do this because when we slide the filter matrix over the input matrix, we might not pick up a matching subset matrix size without going over the boundaries of the input matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectifier function in a CNN (ReLU)\n",
    "\n",
    "Note that ReLU stands for Rectified Linear Unit. It is defined as:\n",
    "$f(x) = max(0,x)$\n",
    "\n",
    "This has the following graph:\n",
    "\n",
    "<img src=\"relu.JPG\">\n",
    "\n",
    "The rectifier works by replacing all _negative_ pixel values in the feature map (result of the convolution) by zero. In this sense it is an element-wise operation. We do this to introduce non-linearity into our ConvNet (otherwise a simple dot product is just a linear transformation). See below for an example of ReLU being applied to an image:\n",
    "\n",
    "<img src=\"relu_image.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling \n",
    "\n",
    "There are different types of pooling: average, max, mode, sum, etc. But it's a method by which to pool together the outputs of a neural network layer. \n",
    "\n",
    "Max pooling works by the following algorithm:\n",
    "1. Define a spatial neighborhood (say 2x2 window)\n",
    "2. Choose a stride size (same idea as above but we apply it to the feature map)\n",
    "3. Take the largest element from the rectified feature map within the window\n",
    "4. Generate a reduced dimensionality of the feature map, this is called the **output map**\n",
    "\n",
    "See below for an example of max and sum pooling being applied to the rectified feature map above:\n",
    "\n",
    "<img src=\"pooling.png\">\n",
    "\n",
    "A major advantage of pooling is that it makes the overal neural network _invariant_ to small transformations. That is, if we suddenly see a large outlier in one of our pixel elements it won't have an outsized impact on the network because it's being pooled with all the other pixel elements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Layer\n",
    "\n",
    "This is the final layer in the neural network. This is a traditional multi layer perceptron and it acts as a classifier for the features generated by the convolution and pooling layers. The popular choice is a softmax function but other classifiers such as SVM can be used as well. \n",
    "\n",
    "For more on multilayer perceptrons see:\n",
    "\n",
    "https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/\n",
    "\n",
    "The term fully connected implies that every neuron in the previous layer is connected to every neuron on the next layer. We use a fully connected to layer to learn a non-linear combination of these features. The result of this layer is a classification probability for each class (if softmax is used). \n",
    "\n",
    "# Backpropagation \n",
    "\n",
    "The training of a CNN has the following algorithm:\n",
    "\n",
    "1. Initialize all filters/parameters/weights with _random_ values\n",
    "2. Pick a training image. Use this as an input to the network and forward propagate the network:\n",
    "    * Convolute the image\n",
    "    * Apply ReLU/Pooling operations\n",
    "    * Classify using the fully connected layer\n",
    "3. Calculate the total error at the output layer:\n",
    "\n",
    "$Total Error = \\frac{1}{2}\\sum (target probability - output probability)^{2}$ \n",
    "\n",
    "4. Use Backpropagation to calculate the gradients (derivative) of the total error w.r.t all weights in the network. Use gradient descent to update all filters/parameters/weight values to minimize the output error. \n",
    "5. Repeat steps 2-4 with all images in the training set\n",
    "\n",
    "The above steps make it such that the CNN has been optimized to correctly classify images from the training set. The idea is that, given a big enough training set that is representative of the \"true\" distribution of images, the trained model shold be able to do a good job at classifying images that it has yet to see."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
