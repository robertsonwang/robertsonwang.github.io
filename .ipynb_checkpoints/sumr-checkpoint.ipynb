{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download a copy of the code [here](https://robertsonwang.github.io/sumr.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Class set up:\n",
    "#TextCleaner() - Contains all the data/methods for cleaning the self.text\n",
    "#SumrGraph() - Contains all the data/methods for creating a summary using TextRank\n",
    "#NaiveSumr() - Contains all the data/methods for creating a summary using naive BOW models\n",
    "#LSASumr() - Contains all the data/methods for creating a summary using Latent Semantic Indexing\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "from collections import Counter\n",
    "from math import fabs #This takes in an absolute value\n",
    "import re\n",
    "from re import split as regex_split, sub as regex_sub, UNICODE as REGEX_UNICODE\n",
    "from itertools import combinations\n",
    "import math\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import itertools\n",
    "\n",
    "class TextCleaner():\n",
    "#This class cleans the text and creates a tf-idf matrix\n",
    "\tdef __init__(self, doc_list):\n",
    "\t\tself.text_dict = {}\n",
    "\t\tself.doc_list = doc_list\n",
    "\t\tself.sentence_list = []\n",
    "\t\tself.tf_idf = None\n",
    "\n",
    "\tdef read_docs(self, base_path = 'quarter_example/'):\n",
    "\t\ttext_dict = {}\n",
    "\n",
    "\t\tfor doc in self.doc_list:\n",
    "\t\t\twith open(base_path + doc) as f:\n",
    "\t\t\t\ttext_dict[doc] = f.read().decode('utf8')\n",
    "\t\t\t\t#Deal with ASCII characters\n",
    "\t\t\t\t#Deal with parenthesis\n",
    "\t\t\t\ttext_dict[doc] = text_dict[doc].replace(u\"\\u2018\", \"'\").replace(u\"\\u2019\", \"'\")\n",
    "\t\t\t\t#Deal with quotes\n",
    "\t\t\t\ttext_dict[doc] = text_dict[doc].replace(u\"\\u201c\", '\"').replace(u\"\\u201d\", '\"')\n",
    "\t\t\t\t#Deal with byte order mark\n",
    "\t\t\t\ttext_dict[doc] = text_dict[doc].replace(u\"\\ufeff\", \"\")\n",
    "\t\t\t\t#Remove new lines\n",
    "\t\t\t\ttext_dict[doc] = text_dict[doc].replace('\\n', '')\n",
    "\n",
    "\t\tself.text_dict = text_dict\n",
    "\t\tself.base_path = base_path\n",
    "\n",
    "\tdef split_sentences(self, text):\n",
    "\t# The regular expression matches all sentence ending punctuation and splits the string at those points.\n",
    "\t# At this point in the code, the list looks like this [\"Hello, world\", \"!\" ... ]. The punctuation and all quotation marks\n",
    "\t# are separated from the actual self.text. The first s_iter line turns each group of two items in the list into a tuple,\n",
    "\t# excluding the last item in the list (the last item in the list does not need to have this performed on it). Then,\n",
    "\t# the second s_iter line combines each tuple in the list into a single item and removes any whitespace at the beginning\n",
    "\t# of the line. Now, the s_iter list is formatted correctly but it is missing the last item of the sentences list. The\n",
    "\t# second to last line adds this item to the s_iter list and the last line returns the full list.\n",
    "\t\tsentences = regex_split(u'(?<![A-Z])([.!?]\"?)(?=\\s+\\\"?[A-Z])',text,flags=REGEX_UNICODE)\n",
    "\t\ts_iter = zip(*[iter(sentences[:-1])] * 2)\n",
    "\t\ts_iter = [''.join(map(unicode,y)).lstrip() for y in s_iter]\n",
    "\t\ts_iter.append(sentences[-1])\n",
    "\t\t\n",
    "\t\treturn s_iter\n",
    "\n",
    "\tdef make_tf_idf(self):\n",
    "\t\t#Create a counter object (term frequency) with words from each document\n",
    "\t\ttf_idf = {}\n",
    "\t\tnum_regex = re.compile('[0-9]')\n",
    "\t\t#Check to see if the text_dict object exists\n",
    "\t\tif self.text_dict:\n",
    "\t\t\ttext_dict = self.text_dict\n",
    "\t\telse:\n",
    "\t\t\ttext_dict = self.read_docs(self.doc_list, self.base_path)\n",
    "\n",
    "\t\tfor doc in text_dict.keys():\n",
    "\t\t\tword_list = self.split_words(text_dict[doc])\n",
    "\t\t\tword_list = [x for x in word_list if not num_regex.match(x)]\n",
    "\t\t\ttf_idf[doc] = dict(Counter(x for x in word_list if x not in stopWords))\n",
    "\t\t\t\n",
    "\t\t#Get the inverse document measure\n",
    "\t\tword_count_dict = {}\n",
    "\t\tdoc_list = tf_idf.keys()\n",
    "\t\tn = len(doc_list)\n",
    "\t\tfor j in range(2, n+1):\n",
    "\t\t\tdoc_combs = [comb for comb in combinations(doc_list, j)]\n",
    "\t\t\tfor doc_comb in doc_combs:\n",
    "\t\t\t\tdoc_set = [set(tf_idf[doc].keys()) for doc in doc_comb]\n",
    "\t\t\t\tcomb_words = set()\n",
    "\t\t\t\tfor word_set in doc_set:\n",
    "\t\t\t\t\tif not comb_words:\n",
    "\t\t\t\t\t\tcomb_words = word_set\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tcomb_words = comb_words.intersection(word_set)\n",
    "\t\t\t\ttry: \n",
    "\t\t\t\t\tword_count_dict[j].append(comb_words)\n",
    "\t\t\t\texcept KeyError:\n",
    "\t\t\t\t\tword_count_dict[j] = list(comb_words)\n",
    "\t\t\t\t\t\n",
    "\t\t#Adjust the tf_idf dict to reflect the new measure\n",
    "\t\tdf = 1\n",
    "\t\tfor doc in tf_idf:\n",
    "\t\t\tfor word in tf_idf[doc].keys():\n",
    "\t\t\t\tfor i in range(n,1,-1):\n",
    "\t\t\t\t\tif word in word_count_dict[i]:\n",
    "\t\t\t\t\t\tdf = i\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\ttf_idf[doc][word] = tf_idf[doc][word] * math.log(float(n)/float(df))\n",
    "\t\t\n",
    "\t\tself.tf_idf = tf_idf\n",
    "\t\treturn tf_idf\n",
    "\n",
    "\tdef split_words(self, text):\n",
    "\t\t#split a string into array of words\n",
    "\t\ttry:\n",
    "\t\t\ttext = regex_sub(r'[^\\w ]', '', text, flags=REGEX_UNICODE)  # strip special chars\n",
    "\t\t\treturn [x.strip('.').lower() for x in text.split()]\n",
    "\n",
    "\t\texcept TypeError:\n",
    "\t\t\tprint \"Error while splitting characters\"\n",
    "\t\t\treturn None\n",
    "\n",
    "class NaiveSumr():\n",
    "#This class is inspired by the tf-idf algorithm put forth in Gong & Liu, 2001. \n",
    "#This also includes other features:\n",
    "#1. sbs,summation based selection, (Hu, Sun and Lim) - Weight sentences greater if they contain more \n",
    "#\"representative words\", i.e. sentences with more keywords are weighted higher\n",
    "#2. dbs, density-based selection - Similair to SBS, weight sentences greater that contain more representative words AND if those words are closer\n",
    "#together in the sentence\n",
    "#3. keywords - These contain manual dictionary entries for terms that are known to be useful for financial stablity/policy making\n",
    "\tdef __init__(self, TextCleaner):\n",
    "\t\tself.TextCleaner = TextCleaner\n",
    "\t\tself.summary_dict = {}\n",
    "\n",
    "\tdef Summarize(self, doc_key):\n",
    "\t\tsummaries = []\n",
    "\t\tsentences = self.TextCleaner.split_sentences(self.TextCleaner.text_dict[doc_key])\n",
    "\t\tsentences = sentences[1:len(sentences)]\n",
    "\t\tkeys = self.keywords(doc_key)\n",
    "\n",
    "\t\tif len(sentences) <= 5:\n",
    "\t\t\treturn sentences\n",
    "\n",
    "\t\t#score setences, and use the top 5 sentences\n",
    "\t\tranks = self.score(sentences, keys).most_common(5)\n",
    "\t\tsum_dict = {sentences.index(a):a for (a,b) in ranks}\n",
    "\n",
    "\t\tfor key in sorted(sum_dict.keys()):\n",
    "\t\t\tsummaries.append(sum_dict[key])\n",
    "\n",
    "\t\tself.summary_dict[doc_key] = summaries\n",
    "\n",
    "\t\treturn summaries\n",
    "\n",
    "\tdef score(self, sentences, keywords):\n",
    "\t\t#score sentences based on different features\n",
    "\t\tranks = Counter()\n",
    "\t\tsenSize = len(sentences)\n",
    "\t\tfor i, s in enumerate(sentences):\n",
    "\t\t\t#i is in the index position in the list and s is the value itself\n",
    "\t\t\tsentence = self.TextCleaner.split_words(s)\n",
    "\t\t\tsentencePosition = self.sentence_position(i+1, senSize)\n",
    "\t\t\tsbsFeature = self.sbs(sentence, keywords)\n",
    "\t\t\tdbsFeature = self.dbs(sentence, keywords)\n",
    "\n",
    "\t\t\t#weighted average of scores from four categories\n",
    "\t\t\ttotalScore = (1.0/2.0 * sbsFeature + 1.0/4.0 * dbsFeature + 1.0/4.0 * sentencePosition) / 4.0\n",
    "\t\t\tranks[s] = totalScore\n",
    "\n",
    "\t\treturn ranks\n",
    "\n",
    "\tdef sentence_position(self, i, size):\n",
    "\t\t\"\"\"different sentence positions indicate different\n",
    "\t\tprobability of being an important sentence\"\"\"\n",
    "\n",
    "\t\tnormalized = i*1.0 / size\n",
    "\t\tif 0 < normalized <= 0.1:\n",
    "\t\t\treturn 0.22\n",
    "\t\telif 0.1 < normalized <= 0.2:\n",
    "\t\t\treturn 0.22\n",
    "\t\telif 0.2 < normalized <= 0.3:\n",
    "\t\t\treturn 0.22\n",
    "\t\telif 0.3 < normalized <= 0.4:\n",
    "\t\t\treturn 0.08\n",
    "\t\telif 0.4 < normalized <= 0.5:\n",
    "\t\t\treturn 0.05\n",
    "\t\telif 0.5 < normalized <= 0.6:\n",
    "\t\t\treturn 0.04\n",
    "\t\telif 0.6 < normalized <= 0.7:\n",
    "\t\t\treturn 0.06\n",
    "\t\telif 0.7 < normalized <= 0.8:\n",
    "\t\t\treturn 0.04\n",
    "\t\telif 0.8 < normalized <= 0.9:\n",
    "\t\t\treturn 0.04\n",
    "\t\telif 0.9 < normalized <= 1.0:\n",
    "\t\t\treturn 0.03\n",
    "\t\telse:\n",
    "\t\t\treturn 0\n",
    "\n",
    "\tdef sbs(self, words, keywords):\n",
    "\t#Return Max(0, \\frac{1}{ \\frac{|# of Words in Sentence * Keyword Score|}{10}}\n",
    "\t\tscore = 0.0\n",
    "\t\tif len(words) == 0:\n",
    "\t\t\treturn 0\n",
    "\t\tfor word in words:\n",
    "\t\t\tif word in keywords:\n",
    "\t\t\t\tscore += keywords[word]\n",
    "\t\treturn (1.0 / fabs(len(words)) * score)/10.0\n",
    "\n",
    "\tdef dbs(self, words, keywords):\n",
    "\t\tif (len(words) == 0):\n",
    "\t\t\treturn 0\n",
    "\t\tsumm = 0\n",
    "\t\tfirst = []\n",
    "\t\tsecond = []\n",
    "\n",
    "\t\tfor i, word in enumerate(words):\n",
    "\t\t\tif word in keywords:\n",
    "\t\t\t\tscore = keywords[word]\n",
    "\t\t\t\tif first == []:\n",
    "\t\t\t\t\tfirst = [i, score]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tsecond = first\n",
    "\t\t\t\t\tfirst = [i, score]\n",
    "\t\t\t\t\tdif = first[0] - second[0]\n",
    "\t\t\t\t\tsumm += (first[1]*second[1]) / (dif ** 2)\n",
    "\n",
    "\t\t# number of intersections\n",
    "\t\tk = len(set(keywords.keys()).intersection(set(words))) + 1\n",
    "\t\treturn (1/(k*(k+1.0))*summ)\n",
    "\n",
    "\tdef keywords(self, doc_key):\n",
    "\t# get the top 10 keywords and their frequency scores\n",
    "\t# ignores blacklisted words in stopWords,\n",
    "\t# counts the number of occurrences of each word. # We should pass this a tf idf matrix trained on the entire quarters corpus!\n",
    "\n",
    "\t\tkeywords = self.TextCleaner.tf_idf[doc_key]\n",
    "\t\tkeywords = {key: value for (key, value) in keywords.items() if value != 0.0}\n",
    "\n",
    "\t\tlm_words = ['loss', 'losses', 'decline', 'declined', 'declines', 'negative', 'lower', 'higher', 'raised', 'lowered']\n",
    "\t\tfed_words = ['foreign', 'exchange', 'interest', 'rates', 'rate', 'environment', 'charge', 'cost', 'tax',\n",
    "\t\t'multicurrency', 'equity', 'markets', 'conditions', 'stable', 'volatile', 'fluctuations', 'conditions',\n",
    "\t\t'risk', 'risky', 'currency', 'currencies', 'credit', 'market', 'VaR', 'VAR', 'capital'\n",
    "\t\t'RWA', 'RWAs', 'restructuring', 'federal', 'reserve', 'LIBOR', 'economy', 'economic', 'charge']\n",
    "\n",
    "\t\tmax_key = max(keywords, key=keywords.get)\n",
    "\n",
    "\t\tfor k in fed_words + lm_words:\n",
    "\t\t\tkeywords[k] = keywords[max_key]\n",
    "\n",
    "\t\treturn keywords\n",
    "\n",
    "class SumrGraph():\n",
    "#This class follows the algorithm put forth in Mihalcea and Tarau, 2004\n",
    "\tdef __init__(self, TextCleaner):\n",
    "\t\tself.TextCleaner = TextCleaner\n",
    "\t\tself.summary_dict = {}\n",
    "\t\tself.graph = None\n",
    "\t\tself.text = \"\"\n",
    "\n",
    "\tdef Summarize(self, doc_key):\n",
    "\t\tself.text = self.TextCleaner.text_dict[doc_key]\n",
    "\t\tself.build_graph()\n",
    "\t\t#This is a modified pagerank (Brin and Page, 1998) http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf\n",
    "\t\t#We use sentence similarity as weights ala TextRank (Mihalcea and Tarau, 2005) https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf\n",
    "\t\tcalculated_page_rank = nx.pagerank(self.graph, weight='weight', alpha = 0.7)\n",
    "\t\tsentences = sorted(calculated_page_rank, key=calculated_page_rank.get,\n",
    "\t\t\t\t   reverse=True)\n",
    "\t\t#Add first sentence of earnings announcement\n",
    "\t\tself.summary_dict[doc_key] = sentences[0:5]\n",
    "\t\t\n",
    "\t\treturn sentences[0:5]\n",
    "\n",
    "\tdef build_graph(self):\n",
    "\t\t# Return a networkx graph instance.\n",
    "\t\t# input nodes: List of hashables that represent the nodes of a graph.\n",
    "\n",
    "\t\tgr = nx.Graph()  # initialize an undirected graph\n",
    "\t\tnodes = self.TextCleaner.split_sentences(self.text) #Create nodes using sentences from document\n",
    "\t\tgr.add_nodes_from(nodes)\n",
    "\t\tnodePairs = list(itertools.combinations(nodes, 2))\n",
    "\n",
    "\t\t# add edges to the graph (weighted by Levenshtein distance)\n",
    "\t\tfor pair in nodePairs:\n",
    "\t\t\tfirstString = pair[0]\n",
    "\t\t\tsecondString = pair[1]\n",
    "\t\t\tlevDistance = self.levenshtein_distance(firstString, secondString)\n",
    "\t\t\tgr.add_edge(firstString, secondString, weight=levDistance)\n",
    "\n",
    "\t\tself.graph = gr\n",
    "\n",
    "\tdef levenshtein_distance(self, first, second):\n",
    "\t\t# Return the inverse Levenshtein distance between two strings.\n",
    "\t\t# We want lower levenshtein distances to imply greater sentence similarity\n",
    "\t\t# Based on:\n",
    "\t\t#\t http://rosettacode.org/wiki/Levenshtein_distance#Python\n",
    "\n",
    "\t\tif len(first) > len(second):\n",
    "\t\t\tfirst, second = second, first\n",
    "\t\tdistances = range(len(first) + 1)\n",
    "\t\tfor index2, char2 in enumerate(second):\n",
    "\t\t\tnew_distances = [index2 + 1]\n",
    "\t\t\tfor index1, char1 in enumerate(first):\n",
    "\t\t\t\tif char1 == char2:\n",
    "\t\t\t\t\tnew_distances.append(distances[index1])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tnew_distances.append(1 + min((distances[index1],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t distances[index1 + 1],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t new_distances[-1])))\n",
    "\t\t\tdistances = new_distances\n",
    "\t\treturn 1.0/float(distances[-1])\n",
    "\n",
    "class LSASumr():\n",
    "# #This class follows the algorithm put forth in Gong & Liu, 2001:\n",
    "# #1. Decompose the document D into individual sentences, call this the candidate set S\n",
    "# #2. Construct the terms by sentence matrix A for document D, rows are terms and columns are sentences\n",
    "# #3. Perform the SVD on A to obtain the singular value matrix E and the right singular vector matrix V^T\n",
    "# #4. Select the k'th right singular vector from matrix V^T\n",
    "# #5. Select the sentence that has the highest loading on the k'th right singular vector\n",
    "# #6. Repeat until you get a summary of desired length\n",
    "# #Recall, SVD decomposes A = UEV^T, where E is a diagonal matrix and U and V^T are orthongonal matrices\n",
    "# #Therefore, AA^T = UEV^TVE^TU^T = UE^2U^T and A^TA = VE^TU^TUEV^T = VE^2V^T.\n",
    "# #E^2 is a diagonal matrix, therefore it must be the case that V contains the eigenvectors of A^TA\n",
    "# #and U contains the eigenvectors of AA^T.\n",
    "# #The elements of A^TA contain the dot products of each term across the document against the individual terms in each sentence,\n",
    "# #Therefore we can think of A^TA as the \"term covariance matrix\" so that projecting the matrix onto a lower dimensional space\n",
    "# #finds the \"latent\" terms or topics that underlie the document.\n",
    "\tdef __init__(self, TextCleaner):\n",
    "\t\tself.TextCleaner = TextCleaner\n",
    "\t\tself.summary_dict = {}\n",
    "\t\tself.term_sentence_matrix = None\n",
    "\t\tself.word_index = None\n",
    "\t\tself.sentence_index = None\n",
    "\t\tself.eigenvalues = None\n",
    "\t\tself.rightsvd = None\n",
    "\t\tself.leftsvd = None\n",
    "\n",
    " \tdef make_term_sentence_matrix(self, doc_key):\n",
    " \t\tnum_regex = re.compile('[0-9]')\n",
    " \t\tsentences = self.TextCleaner.split_sentences(self.TextCleaner.text_dict[doc_key])\n",
    " \t\ttext = self.TextCleaner.text_dict[doc_key]\n",
    " \t\twords = self.TextCleaner.split_words(text)\n",
    "\n",
    " \t\t#Exclude stop words\n",
    " \t\twords = [word for word in words if word not in stopWords]\n",
    " \t\t#Exclude numbers\n",
    " \t\twords = [word for word in words if not num_regex.match(word)]\n",
    "\n",
    " \t\t#Create a unique word dictionary index\n",
    " \t\tself.word_index = {k:v for k,v in zip(range(len(words)), words)}\n",
    " \t\t#Create a sentence index\n",
    " \t\tself.sentence_index = {k:v for k,v in zip(range(len(sentences)), sentences)}\n",
    "\n",
    "\t\tdoc_list = []\n",
    "\t\tfor word in words:\n",
    "\t\t    word_list = []\n",
    "\t\t    df = 0\n",
    "\t\t    #Get term count measure and document frequency measure\n",
    "\t\t    for sentence in sentences:\n",
    "\t\t        sentence_words = self.TextCleaner.split_words(sentence)\n",
    "\t\t        if word in sentence_words:\n",
    "\t\t        \tdf += 1\n",
    "\t\t        \tterm_frequency = float(len([sentence_word for sentence_word in sentence_words if sentence_word == word]))\n",
    "\t\t        \tsentence_length = float(len(sentence))\n",
    "\t\t        \t#Scale term frequency by length of sentence \n",
    "\t\t        \tword_list.append(term_frequency/sentence_length)\n",
    "\t\t        else:\n",
    "\t\t        \tword_list.append(0)\n",
    "\t\t    #Calculate inverse document frequency measure:\n",
    "\t\t    idf = math.log(float(len(sentences))/float(df))\n",
    "\t\t    word_list = [tf * idf for tf in word_list]\n",
    "\t\t    doc_list.append(word_list)\n",
    "\t\tts_matrix = np.matrix(doc_list)\n",
    "\n",
    " \t\tself.term_sentence_matrix = ts_matrix\n",
    "\n",
    " \tdef get_singular_vector(self, doc_key = None):\n",
    " \t\tif self.term_sentence_matrix is not None:\n",
    " \t\t\tu, s, v = np.linalg.svd(self.term_sentence_matrix)\n",
    " \t\telif doc_key is not None:\n",
    " \t\t\tprint \"Making term sentence matrix...\"\n",
    " \t\t\tself.make_term_sentence_matrix(doc_key)\n",
    " \t\telse:\n",
    " \t\t\tprint \"No document passed\"\n",
    " \t\t\treturn None\n",
    "\n",
    " \t\tself.eigenvalues = s\n",
    " \t\tself.rightsvd = v\n",
    " \t\tself.leftsvd = u\n",
    "\n",
    " \tdef Summarize(self, doc_key):\n",
    " \t\t#Select the k'th vector from the right singular vector\n",
    " \t\tk = 5\n",
    " \t\tsummary = []\n",
    " \t\tsentence_indices = []\n",
    "  \t\t#Selecting the \"largest\" loading is actually a subtle assumption \n",
    " \t\t#when it comes to semantic analysis. Do negative loadings\n",
    " \t\t#mean antonyms? What does this mean from a direction interpretation?\n",
    " \t\tfor vec in self.rightsvd[0:k]:\n",
    " \t\t\tsentence_index = abs(vec).argmax()\n",
    " \t\t\tif sentence_index in sentence_indices:\n",
    " \t\t\t\tsentence_index = np.argsort(abs(vec))[0,-2]\n",
    " \t\t\tsentence_indices.append(sentence_index)\n",
    "\n",
    " \t\t#Put the sentences in chronological order\n",
    " \t\tfor sentence in sorted(sentence_indices):\n",
    " \t\t\tsummary.append(self.sentence_index[sentence])\n",
    "\n",
    " \t\tself.summary_dict[doc_key] = summary\n",
    " \t\treturn summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
